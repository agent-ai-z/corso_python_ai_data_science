{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Elaborazione di testi in linguaggio naturale con Python\n",
                "L'elaborazione del linguaggio naturale (NLP) è un campo dell'intelligenza artificiale che si occupa dell'interazione tra computer e linguaggio umano. Utilizzando Python e librerie come **NLTK** (Natural Language Toolkit), possiamo analizzare, comprendere e generare linguaggio umano in modo efficace.\n",
                "\n",
                "Un testo scritto in un linguaggio naturale non ha tag, non ha delimitatori e non ha tipi di dati, ma può comunque essere una ricchissima fonte di informazioni.\n",
                "\n",
                "Esploreremo vari corpus forniti da NLTK, inclusi **Gutenberg, Stopwords, WordNet e Inaugural**, e vedremo come possiamo utilizzare questi strumenti per il processamento del linguaggio naturale."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "vscode": {
                    "languageId": "shellscript"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: nltk in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.9.1)\n",
                        "Requirement already satisfied: click in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (8.1.7)\n",
                        "Requirement already satisfied: joblib in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (1.4.2)\n",
                        "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (2024.11.6)\n",
                        "Requirement already satisfied: tqdm in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (4.67.1)\n",
                        "Requirement already satisfied: colorama in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk) (0.4.6)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install nltk"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Quando installate il modulo NLTK, in realtà installate solo le classi, no i corpus. I corpus sono considerati troppo estesi per poter essere inclusi nella distribuzione. Quando importate il modulo per la prima volta, ricordatevi di richiamare la funzione download() e di installare le parti mancanti, a seconda delle vostre esigenze."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Defaulting to user installation because normal site-packages is not writeable\n",
                        "Requirement already satisfied: nltk in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (3.9.1)\n",
                        "Requirement already satisfied: click in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (8.1.7)\n",
                        "Requirement already satisfied: joblib in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (1.4.2)\n",
                        "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (2024.11.6)\n",
                        "Requirement already satisfied: tqdm in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk) (4.67.1)\n",
                        "Requirement already satisfied: colorama in c:\\users\\gzile\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click->nltk) (0.4.6)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package gutenberg to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package gutenberg is already up-to-date!\n",
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n",
                        "[nltk_data] Downloading package inaugural to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package inaugural is already up-to-date!\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Importazione delle librerie necessarie\n",
                "import nltk\n",
                "\n",
                "from nltk.corpus import gutenberg, stopwords, wordnet, inaugural\n",
                "nltk.download('gutenberg')\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('inaugural')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Che cos'è un Corpus?\n",
                "Un corpus è una collezione strutturata o non strutturata di testi che possono essere utilizzati per l'analisi del linguaggio naturale. In NLTK, tutti i corpus sono disponibili nel modulo `nltk.corpus`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Corpus Gutenberg\n",
                "NLTK fornisce accesso al **Gutenberg Corpus**, una collezione di testi classici della letteratura."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
                    ]
                }
            ],
            "source": [
                "# Lista dei file nel corpus Gutenberg\n",
                "print(gutenberg.fileids())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[Emma by Jane Austen 1816]\n",
                        "\n",
                        "VOLUME I\n",
                        "\n",
                        "CHAPTER I\n",
                        "\n",
                        "\n",
                        "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
                        "and happy disposition, seemed to unite some of the best blessings\n",
                        "of existence; and had lived nearly twenty-one years in the world\n",
                        "with very little to distress or vex her.\n",
                        "\n",
                        "She was the youngest of the two daughters of a most affectionate,\n",
                        "indulgent father; and had, in consequence of her sister's marriage,\n",
                        "been mistress of his house from a very early period.  Her mother\n",
                        "had died t\n"
                    ]
                }
            ],
            "source": [
                "# Lettura del contenuto del primo paragrafo di 'Emma' di Jane Austen\n",
                "emma_text = gutenberg.raw('austen-emma.txt')[:500]\n",
                "print(emma_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stopwords Corpus\n",
                "Le **stopwords** sono parole comuni (come 'il', 'e', 'di') che vengono spesso rimosse nell'elaborazione del linguaggio naturale."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
                    ]
                }
            ],
            "source": [
                "# Lista delle stopwords in lingua inglese\n",
                "stop_words = stopwords.words('english')\n",
                "print(stop_words[:20])  # Mostra le prime 20 stopwords"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### WordNet Corpus\n",
                "WordNet è un lessico per la lingua inglese che collega le parole secondo relazioni semantiche come sinonimi e iponimi."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "happy.a.01 -> enjoying or showing or marked by joy or pleasure\n",
                        "felicitous.s.02 -> marked by good fortune\n",
                        "glad.s.02 -> eagerly disposed to act or to be of service\n",
                        "happy.s.04 -> well expressed and to the point\n"
                    ]
                }
            ],
            "source": [
                "# Sinonimi della parola 'happy'\n",
                "synonyms = wordnet.synsets('happy')\n",
                "for syn in synonyms:\n",
                "    print(syn.name(), '->', syn.definition())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Corpus Inaugural\n",
                "Il corpus **Inaugural** contiene i discorsi inaugurali dei presidenti degli Stati Uniti."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1941-Roosevelt.txt', '1945-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-Bush.txt', '2005-Bush.txt', '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt', '2021-Biden.txt']\n"
                    ]
                }
            ],
            "source": [
                "# Lista dei file nel corpus Inaugural\n",
                "print(inaugural.fileids())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fellow-Citizens of the Senate and of the House of Representatives:\n",
                        "\n",
                        "Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable dec\n"
                    ]
                }
            ],
            "source": [
                "# Leggere le prime righe del discorso di George Washington nel 1789\n",
                "washington_1789 = inaugural.raw('1789-Washington.txt')[:500]\n",
                "print(washington_1789)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Normalizzazione del Testo in Python\n",
                "La normalizzazione del testo è una procedura essenziale nel Natural Language Processing (NLP) che prepara un testo in linguaggio naturale per le successive elaborazioni."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Importazione delle librerie necessarie\n",
                "import re\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import PorterStemmer, WordNetLemmatizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tokenizzazione del Testo\n",
                "I **token** sono le unità fondamentali in cui viene suddiviso un testo durante l'elaborazione del linguaggio naturale. \n",
                "Possono essere parole, frasi o altri elementi significativi.\n",
                "La **tokenizzazione** suddivide il testo in parole o frasi."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['la', 'normalizzazione', 'del', 'testo', 'è', 'fondamentale', 'nel', 'nlp', '!', 'vediamo', 'come', 'funziona', '.']\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt_tab to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt_tab is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "import nltk\n",
                "nltk.download('punkt')\n",
                "nltk.download('punkt_tab')\n",
                "\n",
                "# Testo di esempio\n",
                "testo = \"La normalizzazione del testo è fondamentale nel NLP! Vediamo come funziona.\"\n",
                "\n",
                "# Tokenizzazione in parole\n",
                "tokens = word_tokenize(testo.lower())\n",
                "print(tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Rimozione della Punteggiatura e Stopwords\n",
                "Le **stopwords** sono parole comuni che vengono spesso rimosse per migliorare l'analisi del testo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['normalizzazione', 'testo', 'fondamentale', 'nlp', 'vediamo', 'funziona']\n"
                    ]
                }
            ],
            "source": [
                "# Definizione delle stopwords\n",
                "stop_words = set(stopwords.words('italian'))\n",
                "\n",
                "# Pulizia del testo\n",
                "tokens_filtrati = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
                "print(tokens_filtrati)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stemming\n",
                "Lo **stemming** riduce le parole alla loro radice base."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['normalizzazion', 'testo', 'fondamental', 'nlp', 'vediamo', 'funziona']\n"
                    ]
                }
            ],
            "source": [
                "# Creazione dello stemmer\n",
                "stemmer = PorterStemmer()\n",
                "\n",
                "# Applicazione dello stemming\n",
                "stemmed_words = [stemmer.stem(word) for word in tokens_filtrati]\n",
                "print(stemmed_words)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Lemmatizzazione\n",
                "La **lemmatizzazione** riduce le parole alla loro forma base, tenendo conto del significato."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['normalizzazione', 'testo', 'fondamentale', 'nlp', 'vediamo', 'funziona']\n"
                    ]
                }
            ],
            "source": [
                "# Creazione del lemmatizer\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "# Applicazione della lemmatizzazione\n",
                "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens_filtrati]\n",
                "print(lemmatized_words)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conclusione\n",
                "In questo notebook abbiamo esplorato alcuni dei principali corpus forniti da NLTK, inclusi **Gutenberg, Stopwords, WordNet e Inaugural**. \n",
                "Questi strumenti sono essenziali per il processamento del linguaggio naturale insieme a tecniche di normalizzazione del testo in Python: **tokenizzazione, rimozione delle stopwords, stemming e lemmatizzazione**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
