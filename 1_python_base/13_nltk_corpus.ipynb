{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Elaborazione di testi in linguaggio naturale con Python\n",
                "L'elaborazione del linguaggio naturale (NLP) è un campo dell'intelligenza artificiale che si occupa dell'interazione tra computer e linguaggio umano. Utilizzando Python e librerie come **NLTK** (Natural Language Toolkit), possiamo analizzare, comprendere e generare linguaggio umano in modo efficace.\n",
                "\n",
                "Un testo scritto in un linguaggio naturale non ha tag, non ha delimitatori e non ha tipi di dati, ma può comunque essere una ricchissima fonte di informazioni.\n",
                "\n",
                "Esploreremo vari corpus forniti da NLTK, inclusi **Gutenberg, Stopwords, WordNet e Inaugural**, e vedremo come possiamo utilizzare questi strumenti per il processamento del linguaggio naturale."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "vscode": {
                    "languageId": "shellscript"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: nltk in c:\\users\\gzile\\pycharmprojects\\corso_python\\.venv\\lib\\site-packages (3.9.1)\n",
                        "Requirement already satisfied: click in c:\\users\\gzile\\pycharmprojects\\corso_python\\.venv\\lib\\site-packages (from nltk) (8.1.8)\n",
                        "Requirement already satisfied: joblib in c:\\users\\gzile\\pycharmprojects\\corso_python\\.venv\\lib\\site-packages (from nltk) (1.4.2)\n",
                        "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gzile\\pycharmprojects\\corso_python\\.venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
                        "Requirement already satisfied: tqdm in c:\\users\\gzile\\pycharmprojects\\corso_python\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
                        "Requirement already satisfied: colorama in c:\\users\\gzile\\pycharmprojects\\corso_python\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                }
            ],
            "source": [
                "pip install nltk"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Quando installate il modulo NLTK, in realtà installate solo le classi, no i corpus. I corpus sono considerati troppo estesi per poter essere inclusi nella distribuzione. Quando importate il modulo per la prima volta, ricordatevi di richiamare la funzione download() e di installare le parti mancanti, a seconda delle vostre esigenze."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package gutenberg to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package gutenberg is already up-to-date!\n",
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n",
                        "[nltk_data] Downloading package inaugural to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Unzipping corpora\\inaugural.zip.\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Importazione delle librerie necessarie\n",
                "import nltk\n",
                "from nltk.corpus import gutenberg, stopwords, wordnet, inaugural\n",
                "\n",
                "nltk.download('gutenberg')\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('inaugural')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Che cos'è un Corpus?\n",
                "Un corpus è una collezione strutturata o non strutturata di testi che possono essere utilizzati per l'analisi del linguaggio naturale. In NLTK, tutti i corpus sono disponibili nel modulo `nltk.corpus`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Corpus Gutenberg\n",
                "NLTK fornisce accesso al **Gutenberg Corpus**, una collezione di testi classici della letteratura."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
                    ]
                }
            ],
            "source": [
                "# Lista dei file nel corpus Gutenberg\n",
                "print(gutenberg.fileids())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[The King James Bible]\n",
                        "\n",
                        "The Old Testament of the King James Bible\n",
                        "\n",
                        "The First Book of Moses:  Called Genesis\n",
                        "\n",
                        "\n",
                        "1:1 In the beginning God created the heaven and the earth.\n",
                        "\n",
                        "1:2 And the earth was without form, and void; and darkness was upon\n",
                        "the face of the deep. And the Spirit of God moved upon the face of the\n",
                        "waters.\n",
                        "\n",
                        "1:3 And God said, Let there be light: and there was light.\n",
                        "\n",
                        "1:4 And God saw the light, that it was good: and God divided the light\n",
                        "from the darkness.\n",
                        "\n",
                        "1:5 And God called the light Day, and the darkness he called Night.\n",
                        "And the evening and the morning were the first day.\n",
                        "\n",
                        "1:6 And God said, Let there be a firmament in the midst of the waters,\n",
                        "and let it divide the waters from the waters.\n",
                        "\n",
                        "1:7 And God made the firmament, and divided the waters which were\n",
                        "under the firmament from the waters which were above the firmament:\n",
                        "and it was so.\n",
                        "\n",
                        "1:8 And God called the firmament Heaven. And the evening and the\n",
                        "morning were the second day.\n",
                        "\n",
                        "1:9 And God said, Let the waters under the heav\n"
                    ]
                }
            ],
            "source": [
                "# Lettura del contenuto del primo paragrafo di 'Emma' di Jane Austen\n",
                "emma_text = gutenberg.raw('bible-kjv.txt')[:1000]\n",
                "print(emma_text)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stopwords Corpus\n",
                "Le **stopwords** sono parole comuni (come 'il', 'e', 'di') che vengono spesso rimosse nell'elaborazione del linguaggio naturale."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
                    ]
                }
            ],
            "source": [
                "# Lista delle stopwords in lingua inglese\n",
                "stop_words = stopwords.words('english')\n",
                "print(stop_words[:20])  # Mostra le prime 20 stopwords"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### WordNet Corpus\n",
                "WordNet è un lessico per la lingua inglese che collega le parole secondo relazioni semantiche come sinonimi e iponimi."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "happy.a.01 -> enjoying or showing or marked by joy or pleasure\n",
                        "felicitous.s.02 -> marked by good fortune\n",
                        "glad.s.02 -> eagerly disposed to act or to be of service\n",
                        "happy.s.04 -> well expressed and to the point\n"
                    ]
                }
            ],
            "source": [
                "# Sinonimi della parola 'happy'\n",
                "synonyms = wordnet.synsets('happy')\n",
                "for syn in synonyms:\n",
                "    print(syn.name(), '->', syn.definition())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Corpus Inaugural\n",
                "Il corpus **Inaugural** contiene i discorsi inaugurali dei presidenti degli Stati Uniti."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1941-Roosevelt.txt', '1945-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-Bush.txt', '2005-Bush.txt', '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt', '2021-Biden.txt', '2025-Trump.txt']\n"
                    ]
                }
            ],
            "source": [
                "# Lista dei file nel corpus Inaugural\n",
                "print(inaugural.fileids())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Fellow-Citizens of the Senate and of the House of Representatives:\n",
                        "\n",
                        "Among the vicissitudes incident to life no event could have filled me with greater anxieties than that of which the notification was transmitted by your order, and received on the 14th day of the present month. On the one hand, I was summoned by my Country, whose voice I can never hear but with veneration and love, from a retreat which I had chosen with the fondest predilection, and, in my flattering hopes, with an immutable dec\n"
                    ]
                }
            ],
            "source": [
                "# Leggere le prime righe del discorso di George Washington nel 1789\n",
                "washington_1789 = inaugural.raw('1789-Washington.txt')[:500]\n",
                "print(washington_1789)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Normalizzazione del Testo in Python\n",
                "La normalizzazione del testo è una procedura essenziale nel Natural Language Processing (NLP) che prepara un testo in linguaggio naturale per le successive elaborazioni."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Importazione delle librerie necessarie\n",
                "import re\n",
                "from nltk.tokenize import word_tokenize\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import PorterStemmer, WordNetLemmatizer"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Tokenizzazione del Testo\n",
                "I **token** sono le unità fondamentali in cui viene suddiviso un testo durante l'elaborazione del linguaggio naturale. \n",
                "Possono essere parole, frasi o altri elementi significativi.\n",
                "La **tokenizzazione** suddivide il testo in parole o frasi."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['la', 'normalizzazione', 'del', 'testo', 'è', 'fondamentale', 'nel', 'nlp', '!', 'vediamo', 'come', 'funziona', '.']\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt_tab to\n",
                        "[nltk_data]     C:\\Users\\gzile\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt_tab is already up-to-date!\n"
                    ]
                }
            ],
            "source": [
                "import nltk\n",
                "nltk.download('punkt')\n",
                "nltk.download('punkt_tab')\n",
                "\n",
                "# Testo di esempio\n",
                "testo = \"La normalizzazione del testo è fondamentale nel NLP! Vediamo come funziona.\"\n",
                "\n",
                "# Tokenizzazione in parole\n",
                "tokens = word_tokenize(testo.lower())\n",
                "print(tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Rimozione della Punteggiatura e Stopwords\n",
                "Le **stopwords** sono parole comuni che vengono spesso rimosse per migliorare l'analisi del testo."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['normalizzazione', 'testo', 'fondamentale', 'nlp', 'vediamo', 'funziona']\n"
                    ]
                }
            ],
            "source": [
                "# Definizione delle stopwords\n",
                "stop_words = set(stopwords.words('italian'))\n",
                "\n",
                "# Pulizia del testo\n",
                "tokens_filtrati = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
                "print(tokens_filtrati)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stemming\n",
                "Lo **stemming** riduce le parole alla loro radice base."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['normalizzazion', 'testo', 'fondamental', 'nlp', 'vediamo', 'funziona']\n"
                    ]
                }
            ],
            "source": [
                "# Creazione dello stemmer\n",
                "stemmer = PorterStemmer()\n",
                "\n",
                "# Applicazione dello stemming\n",
                "stemmed_words = [stemmer.stem(word) for word in tokens_filtrati]\n",
                "print(stemmed_words)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Lemmatizzazione\n",
                "La **lemmatizzazione** riduce le parole alla loro forma base, tenendo conto del significato."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "['normalizzazione', 'testo', 'fondamentale', 'nlp', 'vediamo', 'funziona']\n"
                    ]
                }
            ],
            "source": [
                "# Creazione del lemmatizer\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "# Applicazione della lemmatizzazione\n",
                "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens_filtrati]\n",
                "print(lemmatized_words)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Conclusione\n",
                "In questo notebook abbiamo esplorato alcuni dei principali corpus forniti da NLTK, inclusi **Gutenberg, Stopwords, WordNet e Inaugural**. \n",
                "Questi strumenti sono essenziali per il processamento del linguaggio naturale insieme a tecniche di normalizzazione del testo in Python: **tokenizzazione, rimozione delle stopwords, stemming e lemmatizzazione**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
